{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":3312384,"sourceType":"datasetVersion","datasetId":2003133}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, Model\nfrom tensorflow.keras.applications import EfficientNetB0\n\n# Transformer Encoder Block\nclass TransformerEncoder(Model):\n    def __init__(self, embed_dim, num_heads, ff_dim):\n        super(TransformerEncoder, self).__init__()\n        self.attention = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n        self.ffn = tf.keras.Sequential([\n            layers.Dense(ff_dim, activation='relu'),\n            layers.Dense(embed_dim)\n        ])\n        self.add1 = layers.Add()\n        self.norm1 = layers.LayerNormalization(epsilon=1e-6)\n        self.add2 = layers.Add()\n        self.norm2 = layers.LayerNormalization(epsilon=1e-6)\n\n    def call(self, inputs):\n        attn_output = self.attention(inputs, inputs)\n        out1 = self.add1([inputs, attn_output])\n        out1 = self.norm1(out1)\n\n        ffn_output = self.ffn(out1)\n        out2 = self.add2([out1, ffn_output])\n        return self.norm2(out2)\n\n# CNN-Transformer Interaction Module\nclass CTIModule(Model):\n    def __init__(self, cnn_dim, transformer_dim):\n        super(CTIModule, self).__init__()\n        self.cnn_to_transformer = layers.Dense(transformer_dim)\n        self.transformer_to_cnn = layers.Dense(cnn_dim)\n\n    def call(self, cnn_features, transformer_features):\n        cnn_flat = layers.GlobalAveragePooling2D()(cnn_features)\n        cnn_transformed = self.cnn_to_transformer(cnn_flat)\n        cnn_transformed = tf.expand_dims(cnn_transformed, axis=1)\n\n        transformer_pooled = tf.reduce_mean(transformer_features, axis=1)\n        transformer_transformed = self.transformer_to_cnn(transformer_pooled)\n\n        fused_features = cnn_transformed + tf.expand_dims(transformer_transformed, axis=1)\n        return fused_features\n\n# Hybrid Model with split + CTIModule + Conv1D manual block\nclass HybridModelSplitCTI(Model):\n    def __init__(self, num_classes):\n        super(HybridModelSplitCTI, self).__init__()\n        self.cnn = EfficientNetB0(include_top=False, input_shape=(224, 224, 3), weights=\"imagenet\")\n\n        # Projection to embed_dim\n        self.project_to_embed = layers.Dense(128)\n\n        self.transformer_encoder = TransformerEncoder(embed_dim=128, num_heads=8, ff_dim=256)\n\n        # Manual efficient block with Conv1D\n        self.manual_efficient_conv1d = layers.Conv1D(64, kernel_size=3, activation='relu', padding='same')\n        self.manual_efficient_pool = layers.GlobalAveragePooling1D()\n        self.manual_efficient_dense = tf.keras.Sequential([\n            layers.Dense(256, activation='relu'),\n            layers.Dense(128, activation='relu')\n        ])\n\n        # CTI Module integration\n        self.cti_module = CTIModule(cnn_dim=128, transformer_dim=128)\n\n        self.classifier = tf.keras.Sequential([\n            layers.Dense(256, activation='relu'),\n            layers.Dense(num_classes, activation='softmax')\n        ])\n\n    def call(self, inputs, return_all=False):\n        cnn_features = self.cnn(inputs)  # (batch, H, W, 1280)\n\n        # Flatten and project\n        batch_size = tf.shape(cnn_features)[0]\n        flatten_tokens = tf.reshape(cnn_features, [batch_size, -1, cnn_features.shape[-1]])  # (batch, tokens, 1280)\n        flatten_tokens_proj = self.project_to_embed(flatten_tokens)  # (batch, tokens, 128)\n\n        # Split tokens\n        total_tokens = tf.shape(flatten_tokens_proj)[1]\n        split_point = total_tokens // 2\n\n        q1 = flatten_tokens_proj[:, :split_point, :]  # Q1 → Transformer\n        q2 = flatten_tokens_proj[:, split_point:, :]  # Q2 → Manual block\n\n        # Process Q1 through transformer\n        q1_out = self.transformer_encoder(q1)\n\n        # Process Q2 through manual efficient block with Conv1D\n        q2_conv = self.manual_efficient_conv1d(q2)  # (batch, tokens, 64)\n        q2_pooled = self.manual_efficient_pool(q2_conv)  # (batch, 64)\n        q2_out = self.manual_efficient_dense(q2_pooled)  # (batch, 128)\n\n        # CTI module fusion (cnn_features + transformer features)\n        cti_out = self.cti_module(cnn_features, q1_out)\n        cti_out_pooled = tf.reduce_mean(cti_out, axis=1)\n\n        # Fuse CTI output with manual efficient output\n        fused = tf.concat([cti_out_pooled, q2_out], axis=-1)\n\n        output = self.classifier(fused)\n\n        if return_all:\n            return {\n                'cnn_features': cnn_features,\n                'flatten_tokens_proj': flatten_tokens_proj,\n                'q1': q1,\n                'q1_out': q1_out,\n                'q2': q2,\n                'q2_conv': q2_conv,\n                'q2_pooled': q2_pooled,\n                'q2_out': q2_out,\n                'cti_out': cti_out,\n                'cti_out_pooled': cti_out_pooled,\n                'fused': fused,\n                'final_output': output\n            }\n        else:\n            return output\n\n# === Instantiate and print summary ===\nnum_classes = 8  # Replace with your dataset class count\ninput_shape = (224, 224, 3)\n\ninputs = tf.keras.Input(shape=input_shape)\nmodel_cti = HybridModelSplitCTI(num_classes=num_classes)\noutputs = model_cti(inputs)\nfinal_model_cti = tf.keras.Model(inputs=inputs, outputs=outputs)\n\nfinal_model_cti.summary()\n\n# === Test intermediate outputs ===\n\nbatch = tf.random.normal((8, 224, 224, 3))\noutputs_all = model_cti(batch, return_all=True)\n\nfor key, value in outputs_all.items():\n    print(f\"{key}: shape {value.shape}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T08:44:52.138565Z","iopub.execute_input":"2025-07-07T08:44:52.138805Z","iopub.status.idle":"2025-07-07T08:44:57.361955Z","shell.execute_reply.started":"2025-07-07T08:44:52.138789Z","shell.execute_reply":"2025-07-07T08:44:57.361345Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional_47\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_47\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ input_layer_62 (\u001b[38;5;33mInputLayer\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ hybrid_model_split_cti_14            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)                   │       \u001b[38;5;34m5,129,963\u001b[0m │\n│ (\u001b[38;5;33mHybridModelSplitCTI\u001b[0m)                │                             │                 │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ input_layer_62 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ hybrid_model_split_cti_14            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)                   │       <span style=\"color: #00af00; text-decoration-color: #00af00\">5,129,963</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">HybridModelSplitCTI</span>)                │                             │                 │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,129,963\u001b[0m (19.57 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,129,963</span> (19.57 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,087,940\u001b[0m (19.41 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,087,940</span> (19.41 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m42,023\u001b[0m (164.16 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">42,023</span> (164.16 KB)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"cnn_features: shape (8, 7, 7, 1280)\nflatten_tokens_proj: shape (8, 49, 128)\nq1: shape (8, 24, 128)\nq1_out: shape (8, 24, 128)\nq2: shape (8, 25, 128)\nq2_conv: shape (8, 25, 64)\nq2_pooled: shape (8, 64)\nq2_out: shape (8, 128)\ncti_out: shape (8, 1, 128)\ncti_out_pooled: shape (8, 128)\nfused: shape (8, 256)\nfinal_output: shape (8, 8)\n","output_type":"stream"}],"execution_count":15}]}